{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebfec8f5-f048-4814-b0af-8f77d117260a",
   "metadata": {},
   "source": [
    "This script runs inference on a tile, using the specified model. If requested, it will also save the predictions to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb73e8-e934-48ad-83c3-44ee5023aba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rasterio\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import utils.data as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8871a-d818-4cbf-ad96-c29253569aeb",
   "metadata": {},
   "source": [
    "The block below includes all the parameters for inference. We can pass in different parameters using `papermill`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe68d40-cc01-46bd-9a18-6b97d0780b1e",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# some parameters about the datasets on which to perform inference\n",
    "paths = [\n",
    "    \"/datadrive/snake/lakes/le7-2015/splits/train/GL083789E28642N-20150801.tif\", \n",
    "    \"/datadrive/snake/lakes/le7-2015/splits/train/GL087427E28754N-20151101.tif\"\n",
    "]\n",
    "stats = \"/datadrive/snake/lakes/le7-2015/splits/train/processed/statistics.csv\"\n",
    "dataset_opts = {}\n",
    "\n",
    "# path to the model to load, and default options\n",
    "model_fn = \"/datadrive/snake/glaciers/test_models/snake_200.pt\"\n",
    "model_opts = {\"model\": \"DELSE\", \"pth_model\": \"/datadrive/snake/lakes/models/MS_DeepLab_resnet_trained_VOC.pth\"}\n",
    "\n",
    "# Where to save predictions? If None, doesn't save anything\n",
    "out_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac27a0-74d1-4b0e-9a9b-df19810900f4",
   "metadata": {},
   "source": [
    "The block below loads the model that we've previously trained. Any options that were used to customize the model architecture need to be passed in through the `model_opts` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1daee-7cce-4485-8204-d95c41700224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.unet import UnetModel\n",
    "from models.networks import backend_cnnn\n",
    "device = torch.device(cuda if torch.cuda.is_available() else cpu)\n",
    "\n",
    "# specify the model, and load the state dict\n",
    "if model_opts[\"model\"] == \"U-Net\":\n",
    "    model = UnetModel(**model_opts)\n",
    "elif model_opts[\"model\"] == \"DELSE\":\n",
    "    model = backend_cnn(**model_opts)\n",
    "\n",
    "model.load_state_dict(model_fn)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ea3f2-e589-4aa7-bcfc-2d258afa5de1",
   "metadata": {},
   "source": [
    "Next, we construct a dataloader over all the paths that we want to perform inference on. I'm assuming here that the dataset class knows how to handle any imputation and renormalization. I'm also assuming that it will not crop the input image. If the dataset class doesn't have that much flexibility, then we can just create a small `inference` dataset class that will just do the imputation and renormalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b6ac1-14d2-4923-ba5b-5efc777ff3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.dataset import DataLoader\n",
    "\n",
    "# create a dataloader, and perform inference\n",
    "dataset = DataSet(paths, **dataset_opts)\n",
    "loader = DataLoader(dataset)\n",
    "ix = 0\n",
    "with torch.no_grad():\n",
    "    for x, _ in loader:\n",
    "        y_hat = model(x.to(device))\n",
    "        if out_dir is not None:\n",
    "            for i in range(y_hat.shape[0]):\n",
    "                out_path = out_dir / f\"{dataset.ids[ix]}-pred.tif\"\n",
    "                dt.save_raster(y_hat[i], x.meta, x.transform, out_path)\n",
    "                ix += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304eda96-0d5c-4fe1-bc63-9f1da705b1ce",
   "metadata": {},
   "source": [
    "Finally, it can be nice to look at a few images directly in the notebook, just so we don't have to copy all the tif's and load QGIS to see a couple predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598dac62-2183-4dd2-b239-5ad3d876f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(y_hat.shape[0]):\n",
    "    x_ = np.transpose(x[i], (1, 2, 0))\n",
    "    plt.imshow(10 * x_ / np.nanmax(x_))\n",
    "    plt.imshow(y_hat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b1759-cba0-4ba9-9fb6-e5080569ad90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mappingvis",
   "language": "python",
   "name": "mappingvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
