{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148fa5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import csv\n",
    "import shutil\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import utils.data as dt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5c91e",
   "metadata": {},
   "source": [
    "The three parameters of this script are,\n",
    "\n",
    "* Which directory contains all the scenes that we want to extract preprocessing-relevant summaries and masks for?\n",
    "* Where do we want to save our results?\n",
    "* What is the full path to the labels from which we will construct our masks?\n",
    "\n",
    "For the first question, we will use the following choices (on the Azure machine),\n",
    "\n",
    "* Bing Recent: `/datadrive/glaciers/bing_glaciers/bing_glacial_lakes`\n",
    "* Landsat 2015: `/datadrive/snake/lakes/le7-2015/{split}`\n",
    "* Landsat all: `/datadrive/snake/lakes/imagery`\n",
    "\n",
    "Technically we only need Landsat all, but Landsat 2015 is convenient because that's the only data that we can train on (the rest would be for purely inference purposes).\n",
    "\n",
    "Landsat:\n",
    "```\n",
    "papermill -p in_dir /datadrive/snake/lakes/le7-2015/splits/train -p out_dir /datadrive/snake/lakes/le7-2015/splits/train/processed preprocess.ipynb -\n",
    "papermill -p in_dir /datadrive/snake/lakes/le7-2015/splits/val -p out_dir /datadrive/snake/lakes/le7-2015/splits/val/processed preprocess.ipynb -\n",
    "papermill -p in_dir /datadrive/snake/lakes/le7-2015/splits/test -p out_dir /datadrive/snake/lakes/le7-2015/splits/test/processed preprocess.ipynb -\n",
    "```\n",
    "\n",
    "Bing:\n",
    "```\n",
    "papermill -p in_dir /datadrive/glaciers/bing_glaciers/bing_glacial_lakes/splits/train -p out_dir /datadrive/glaciers/bing_glaciers/bing_glacial_lakes/splits/train/processed preprocess.ipynb -\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0bb44f",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "in_dir = \"/datadrive/snake/lakes/le7-2015/splits/train\"\n",
    "label_path = \"/datadrive/snake/lakes/GL_3basins_2015.shp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\"in\": Path(in_dir), \"label\": Path(label_path)}\n",
    "if (paths[\"in\"] / \"images\").exists():\n",
    "    shutil.rmtree(paths[\"in\"] / \"images\")\n",
    "    shutil.rmtree(paths[\"in\"] / \"labels\")\n",
    "(paths[\"in\"] / \"labels\").mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc026e",
   "metadata": {},
   "source": [
    "Next, we read in the label data and set up the writer to which we will save summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e031a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_list = list(paths[\"in\"].glob(\"*.tif\"))\n",
    "y = gpd.read_file(paths[\"label\"])\n",
    "fields = [\"scene\"] + sum([[f\"{s}_{i}\" for i in range(11)] for s in [\"mean\", \"sdev\"]], [])\n",
    "f = open(paths[\"in\"] / \"statistics.csv\", \"a\")\n",
    "writer = csv.writer(f)\n",
    "writer.writerow(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a736a",
   "metadata": {},
   "source": [
    "Finally, we can loop over all the scenes in `in_dir` and save the relevant statistics and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c588591",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scene in scene_list:\n",
    "    img = rasterio.open(scene)\n",
    "    result = dt.preprocessor(img, y)\n",
    "    out_path = paths[\"in\"] / f\"labels/{scene.stem}-labels.tif\"\n",
    "    dt.save_raster(result[2], img.meta, img.transform, out_path)\n",
    "    writer.writerow([str(scene.stem)] + list(np.hstack(result[:2])))\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(paths[\"in\"] / \"images\").mkdir(parents=True)\n",
    "[shutil.move(str(s), paths[\"in\"] / \"images\") for s in scene_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for s in result[2]:\n",
    "    plt.imshow(s)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e7a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mappingvis",
   "language": "python",
   "name": "mappingvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
